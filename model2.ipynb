{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NEWEST Olivers copy of Project22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmWnDQJK8o6t"
      },
      "source": [
        "import pickle\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxQNJbYhb2Ml"
      },
      "source": [
        "###Mount data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "kX8xBx8Kb1ZF",
        "outputId": "f999e264-43e0-4f17-f602-1378d9bfc1bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g7bZe7HvJncZjjRGrIR8lN_qcDii86Vk5TK7T_6sICT3gg5GqulvOc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a145c0899d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mdrive_exited\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0moauth_failed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mdomain_disabled_drivefs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     ])\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         return self.expect_list(compiled_pattern_list,\n\u001b[0;32m--> 344\u001b[0;31m                 timeout, searchwindowsize, async_)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     def expect_list(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl3QUyNacV4a"
      },
      "source": [
        "#!ls drive/'My Drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4YgBvBtcWRN"
      },
      "source": [
        "drive_path = 'drive/My Drive/Project22'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAKB4KLUr89R"
      },
      "source": [
        "print(os.getcwd())\n",
        "#if not os.getcwd() == '/content/drive/My Drive/Project22':\n",
        "os.chdir(drive_path)\n",
        "#  os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asz2KvHIbYhV"
      },
      "source": [
        "pip install mido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdSMWDU6HGL4"
      },
      "source": [
        "# GLOBALS\n",
        "\n",
        "SEQ_LENGTH = 256\n",
        "TARGET_LENGTH = 1\n",
        "NUM_CLASSES = 135\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwKLzTBlD5t9"
      },
      "source": [
        "\n",
        "\n",
        "### Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9YQGGJAHOxw"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from math import floor\n",
        "from midi_conversion import convert_midi_to_numpy, convert_matrix_to_word_seq, convert_to_number_seq\n",
        "from torch.utils import data\n",
        "import random\n",
        "from time import time\n",
        "\n",
        "#from PIL import Image, ImageOps\n",
        "#from skimage.io import imread\n",
        "#from skimage.transform import resize\n",
        "#from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import warnings\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN8YxxHIJw62"
      },
      "source": [
        "    \"\"\"class Dataset(data.Dataset): \n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the size of the dataset\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieve inputs and targets at the given index\n",
        "        X = self.inputs[index]\n",
        "        y = self.targets[index]\n",
        "        return X, y\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFNfCflcD-8J"
      },
      "source": [
        "class load_data():\n",
        "    # data_train, data_test and le are public\n",
        "    def __init__(self, print_status = False, ticks_per_beat = 12):\n",
        "        track_paths = self._generate_paths()\n",
        "        self.ticks_per_beat = ticks_per_beat\n",
        "        self.print_status = print_status\n",
        "        self._load(track_paths)\n",
        "\n",
        "    def _generate_paths(self):\n",
        "        #allowed_folders = [\"2004\",\"2006\",\"2008\",\"2009\",\"2011\",\"2013\",\"2014\",\"2015\",\"2017\",\"2018\"]\n",
        "        allowed_folders = [\"2004\",\"2006\",\"2008\",\"2009\"]\n",
        "        paths = []\n",
        "        maestro_dir = \"./maestro-v2.0.0/\"\n",
        "        for maestro_folder in os.listdir(maestro_dir):\n",
        "            if maestro_folder in allowed_folders:\n",
        "                for track_path in os.listdir(maestro_dir + maestro_folder):\n",
        "                    paths.append(maestro_dir + maestro_folder +\n",
        "                                 \"/\" + track_path)\n",
        "                  \n",
        "        return paths\n",
        "\n",
        "    def _load(self, track_paths):\n",
        "        test_fraction = 0.1\n",
        "        self.train_data = []\n",
        "        self.test_data = []\n",
        "        # debug\n",
        "        t0 = time()\n",
        "        load_times = []\n",
        "        tracks_done = 0\n",
        "\n",
        "        # the real dataloader\n",
        "        for path in track_paths:\n",
        "            t1 = time() #debug\n",
        "            track = convert_midi_to_numpy(path, ticks_per_beat = self.ticks_per_beat) #convert to vector representation\n",
        "            track = convert_matrix_to_word_seq(track) #convert to word seq\n",
        "            track = convert_to_number_seq(track) #convert to number representation\n",
        "            track_np = np.array(track) #convert to numpy\n",
        "            if random.random() > test_fraction: self.train_data.append(track_np) # add to train\n",
        "            else: self.test_data.append(track_np) # add to test\n",
        "            #debug\n",
        "            tracks_done += 1\n",
        "            t2 = time()\n",
        "            load_time = t2-t1\n",
        "            load_times.append(load_time)\n",
        "            tracks_left = len(track_paths) - tracks_done\n",
        "            load_time_avr = sum(load_times)/len(load_times)\n",
        "            if self.print_status: print(path[-20:],\"took\",str(load_time)[:4],\"s,\",tracks_done,\"out of\",len(track_paths),\": time left\", str((tracks_left*load_time_avr)/60)[:4],\"minutes\")\n",
        "        \n",
        "        \"\"\"\n",
        "        #test dataloader\n",
        "        for i in range(250):\n",
        "            track_length = 1000\n",
        "            note = random.randint(0,128)\n",
        "            track = np.zeros((track_length,129))\n",
        "            for e in range(track_length): #line\n",
        "                track[e][note] = 1\n",
        "            train_data.append(track)\n",
        "            test_data.append(track)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"train:\",len(self.train_data))\n",
        "        print(\"test:\",len(self.test_data))\n",
        "\n",
        "        #cut to 1024 pieces (snack pieces)\n",
        "        #self.train_cut = self._chop_data(self.train_uncut)\n",
        "        #self.test = self._chop_data(self.test_uncut)\n",
        "        #self.train_targets = self._chop_data(self.train_uncut_shifted)\n",
        "        #self.test_targets = self._chop_data(self.test_uncut_shifted)\n",
        "        #self.train = Dataset(self.train_cut, self.train_targets)\n",
        "\n",
        "    def generate_cut_data(self, cut_length = 128):\n",
        "        self.cut_length = cut_length\n",
        "        #chop and generate targets\n",
        "        train_data_cut, train_data_cut_targets = self._chop_data(self.train_data)\n",
        "        test_data_cut, test_data_cut_targets = self._chop_data(self.test_data)\n",
        "\n",
        "        #convert to numpy, rename data and add extra pseduo dimension\n",
        "        self.train_cut = np.expand_dims(np.array(train_data_cut),2)\n",
        "        self.train_cut_targets = np.array(train_data_cut_targets)\n",
        "        self.test_cut = np.expand_dims(np.array(test_data_cut),2)\n",
        "        self.test_cut_targets = np.array(test_data_cut_targets)\n",
        "\n",
        "    def _shift_data(self,data, n=1):\n",
        "        data = np.roll(data, -n, axis = 0)\n",
        "        data[-1] = 0 #makes the very last number 0 to keep the correct size\n",
        "        return data\n",
        "\n",
        "    def _chop_data(self, uncut_data):\n",
        "        seq_length = self.cut_length\n",
        "        all_tracks_cut = []\n",
        "        all_tracks_cut_targets = []\n",
        "        for track in uncut_data:\n",
        "            track_pieces_length = floor(len(track)/seq_length)\n",
        "            leftover_length = len(track)%seq_length\n",
        "            #cut track\n",
        "            for i in range(track_pieces_length):\n",
        "                if (i+1)*seq_length+TARGET_LENGTH > len(track): break #if its too short for creating target, stop\n",
        "                all_tracks_cut.append(track[i*seq_length:(i+1)*seq_length])\n",
        "                all_tracks_cut_targets.append(track[(i+1)*seq_length : (i+1)*seq_length+TARGET_LENGTH]) #first part of next cut\n",
        "            #and the tail. adds trailing zeroes to make up for missing length \n",
        "            #ACUTALLY WHATEVER THE TAIL ITS JUST ZEROES EITHER WAY ---\n",
        "            #leftover = track[seq_length*track_pieces_length:] + [0 for _ in range(track_pieces_length - leftover_length)]\n",
        "            #cut_track.append(leftover)\n",
        "\n",
        "        return all_tracks_cut, all_tracks_cut_targets\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd78xepsG6Nv"
      },
      "source": [
        "class batch_generator():\n",
        "    def __init__(self, data, batch_size=64,\n",
        "                 num_iterations=5e3, num_features=1, seed=42, val_size=0.1):\n",
        "        self._train = data.train_cut\n",
        "        self._train_targets = data.train_cut_targets\n",
        "        self._test = data.test_cut #TODO WE DON'T USE THIS YET\n",
        "        self._test_targets = data.test_cut_targets #TODO WE DON'T USE THIS YET\n",
        "\n",
        "        self._batch_size = batch_size\n",
        "        self._seq_length = SEQ_LENGTH\n",
        "        self._num_iterations = num_iterations\n",
        "        self._num_features = num_features\n",
        "        self._seed = seed\n",
        "        self._val_size = val_size\n",
        "        self._valid_split()\n",
        "\n",
        "    def _valid_split(self):\n",
        "        p = self._val_size\n",
        "        val_filter = [True if p < random.random() else False for _ in range(len(self._train))] #some filter to choose valid from\n",
        "        train_filter = [not val for val in val_filter]\n",
        "        #while True: a = 0 #nice debug break\n",
        "        self._idcs_valid = np.arange(len(self._train))[val_filter]\n",
        "        self._idcs_train = np.arange(len(self._train))[train_filter]\n",
        "\n",
        "    def _shuffle_train(self):\n",
        "        np.random.shuffle(self._idcs_train)\n",
        "\n",
        "    def _batch_init(self, purpose):\n",
        "        assert purpose in ['train', 'valid', 'test']\n",
        "\n",
        "        batch_holder = dict()\n",
        "\n",
        "        batch_holder['inputs'] = np.zeros(\n",
        "            (self._batch_size, self._seq_length, 1), dtype=\"float32\")\n",
        "        batch_holder['targets'] = np.zeros(\n",
        "            (self._batch_size, TARGET_LENGTH), dtype=\"float32\")\n",
        "        return batch_holder\n",
        "\n",
        "    def gen_valid(self):\n",
        "        batch = self._batch_init(purpose='valid')\n",
        "        i = 0\n",
        "        for idx in self._idcs_valid:\n",
        "            batch['inputs'][i] = self._train[idx]\n",
        "            batch['targets'][i] = self._train_targets[idx]\n",
        "            i += 1\n",
        "            if i >= self._batch_size:\n",
        "                yield batch, i\n",
        "                batch = self._batch_init(purpose='valid')\n",
        "                i = 0\n",
        "        #if i != 0:\n",
        "        #    batch = batch[:i]\n",
        "        #    yield batch, i\n",
        "\n",
        "    def gen_test(self): #TODO THIS IS PROBABLY BROKEN FOR NOW\n",
        "        batch = self._batch_init(purpose='test')\n",
        "        i = 0\n",
        "        for idx in range(len(self._test)):\n",
        "            batch[i] = self._test[idx]\n",
        "            i += 1\n",
        "            if i >= self._batch_size:\n",
        "                yield batch, i\n",
        "                batch = self._batch_init(purpose='test')\n",
        "                i = 0\n",
        "        if i != 0:\n",
        "            yield batch, i\n",
        "\n",
        "    def gen_train(self):\n",
        "        batch = self._batch_init(purpose='train')\n",
        "        iteration = 0\n",
        "        i = 0\n",
        "        while True:\n",
        "            # shuffling all batches between batches. Shuffles idcs so it keeps the targets\n",
        "            self._shuffle_train()\n",
        "            for idx in self._idcs_train:\n",
        "                # extract data from dict\n",
        "                batch['inputs'][i] = self._train[idx]\n",
        "                batch['targets'][i] = self._train_targets[idx]\n",
        "                i += 1\n",
        "                if i >= self._batch_size:\n",
        "                    yield batch\n",
        "                    batch = self._batch_init(purpose='train')\n",
        "                    i = 0\n",
        "                    iteration += 1\n",
        "                    if iteration >= self._num_iterations:\n",
        "                        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1zmtQc8Eudi"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm4-7ibcEs3U"
      },
      "source": [
        "data = load_data(print_status = True) # UNCOMMENT FOR DATA LOADER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbNHV-9ZKXHC"
      },
      "source": [
        "#data.train_data[0][:10] #DEBUG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40feO9mDPe_o"
      },
      "source": [
        "# CUT DATA\n",
        "data.generate_cut_data(SEQ_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG_ZKjgBKq7p"
      },
      "source": [
        "#i = 4124\n",
        "#print(data.train_cut[i])\n",
        "#print(data.train_cut_targets[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z3bCCd1QcMV"
      },
      "source": [
        "# DATA DEBUG\n",
        "#data.train_data = [list(range(457)) for _ in range(10000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QX7DvqS6YPt"
      },
      "source": [
        "# PICKLE STUFF\n",
        "# Dump the data into a pickle file\n",
        "\"\"\"\n",
        "with open('data.pickle_half_tracks_cut', 'wb') as f:\n",
        "  pickle.dump(data, f)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open('data.pickle_half_tracks_cut', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00nDPAaY0n2c"
      },
      "source": [
        "#Some debugging stuff\n",
        "#data.train_cut[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBOx7NDQ1L4I"
      },
      "source": [
        "\"\"\"for o in range(257,300):\n",
        "    n = np.count_nonzero(data.train_cut == o)\n",
        "    s = data.train_cut.size\n",
        "    print((n/s)*100,\"% is \",o,\" i.e. wait\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "436NVHtND1qm"
      },
      "source": [
        "np.count_nonzero(data.train_cut > 120)/data.train_cut.size*100 # % percents are wait"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2TzXO_5-W8t"
      },
      "source": [
        "#data.train_data = old_train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL_WjYf2-ElT"
      },
      "source": [
        "# DISTRUBUTE WAITS OVER RANGE\n",
        "\"\"\"for i, track in enumerate(data.train_data):\n",
        "    c = 0\n",
        "    for e, num in enumerate(track):\n",
        "        if num > 258: # is wait\n",
        "            data.train_data[i][e] = random.randint(259,300)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QLeG4__AQZt"
      },
      "source": [
        "\n",
        "np.unique(data.train_cut)\n",
        "\n",
        "data.train_cut.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo_432_j_meL"
      },
      "source": [
        "np.max(data.train_cut)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUA0ULXh1mYU"
      },
      "source": [
        "data_distr = np.bincount(data.train_cut.reshape((-1)))\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "x = list(range(len(data_distr)))\n",
        "y = data_distr\n",
        "ax.bar(x,y)\n",
        "axes = plt.gca()\n",
        "#axes.set_ylim([0,100000])\n",
        "#axes.set_xlim([0,120])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ctraGMi9oEU"
      },
      "source": [
        "# and its corresponding target\n",
        "#plot_midi(data.train_cut_targets[10][:],seq_length = TARGET_LENGTH)\n",
        "#%debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSD5Fjv7e7wW"
      },
      "source": [
        "#mone = np.full((151337, 1), 1)\n",
        "#data.train_cut = np.subtract(data.train_cut, mone)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO36G3oCs15C"
      },
      "source": [
        "#np.unique(data.train_cut_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK8QmZtW7_s1"
      },
      "source": [
        "### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HOK8Fk_8Eqa"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d, BatchNorm2d, ReLU\n",
        "from torch.nn.functional import relu, elu, relu6, tanh, softmax\n",
        "\n",
        "lstm_hidden = 50\n",
        "seq_length = SEQ_LENGTH\n",
        "OUT_CLASSES = 135\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "\n",
        "        self.lstm_1 = nn.LSTM(input_size=1,\n",
        "                              hidden_size=lstm_hidden,\n",
        "                              num_layers=1,\n",
        "                              bidirectional=False,\n",
        "                              batch_first = True)\n",
        "\n",
        "\n",
        "        self.l_hidden = Linear(in_features = lstm_hidden*seq_length,\n",
        "                                    out_features = OUT_CLASSES,\n",
        "                                    bias=False)\n",
        "        \"\"\"\n",
        "\n",
        "        self.l_hidden = Linear(in_features = seq_length,\n",
        "                                    out_features = SEQ_LENGTH*50,\n",
        "                                    bias=False)\n",
        "    \n",
        "        self.l_hidden2 = Linear(in_features = SEQ_LENGTH*50,\n",
        "                                    out_features = NUM_CLASSES,\n",
        "                                    bias=False)\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        self.dropout = Dropout(0.1)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \n",
        "        #features = []\n",
        "        out = {}\n",
        "        \n",
        "\n",
        "        #x, (h, c) = self.lstm_1(x)\n",
        "        #x,_ = self.lstm_1(inp)\n",
        "\n",
        "        #print(\"x\",x.shape)\n",
        "        #print(\"h\",h)\n",
        "        #print(\"c\",c)\n",
        "        #print(\"----\")\n",
        "\n",
        "        #features.append(x)\n",
        "        # Final concatenated data processing\n",
        "        #features_final = torch.cat(features, dim=1)\n",
        "\n",
        "\n",
        "        #print(\"x0\",x)\n",
        "\n",
        "        #x = torch.reshape(x,(-1,3))\n",
        "        #x = torch.transpose(x,1,0)\n",
        "\n",
        "        x, _ = self.lstm_1(inp)\n",
        "        #x = inp\n",
        "        x = torch.reshape(x,(x.shape[0],-1))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.l_hidden(x))\n",
        "        #x = torch.sigmoid(self.l_hidden2(x))\n",
        "        out['out'] = np.squeeze(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "use_cuda = torch.cuda.is_available() #TODO ALLOW AGAIN\n",
        "net = Net()\n",
        "if use_cuda:\n",
        "    net.cuda()\n",
        "\n",
        "print(net)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAqBH2J_7Bdi"
      },
      "source": [
        "# a small sanity check\n",
        "#%debug\n",
        "\n",
        "inp = torch.randn(500, SEQ_LENGTH, 1).cuda()\n",
        "\n",
        "outp,hidden = net(inp)\n",
        "outp,hidden = net(inp)\n",
        "\n",
        "for key in outp:\n",
        "    print(key,outp[key].shape)\n",
        "    #print(outp[key][0][1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2zNAXLf8_g5"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-_eODrbZxqG"
      },
      "source": [
        "from numpy.lib.npyio import load\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d, BatchNorm2d, ReLU\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from math import log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w03LJSGf9CUL"
      },
      "source": [
        "VALIDATION_SIZE = 0.1  # 0.1 is ~ 100 samples for validation\n",
        "max_iter = 2000\n",
        "log_every = 100\n",
        "eval_every = 100\n",
        "batch_size = 500\n",
        "\n",
        "net = Net()\n",
        "if use_cuda:\n",
        "    net.cuda()\n",
        "\n",
        "def simple_crit(inputs,targets):\n",
        "    b = torch.abs(torch.subtract(inputs,targets)-0.001)\n",
        "    a = -torch.log(b)\n",
        "    return torch.mean(a)\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "#criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
        "\n",
        "#normie weights\n",
        "#loss_weights = torch.zeros(135)\n",
        "#loss_weights[0:62*2] = 1\n",
        "#loss_weights[62*2:] = 0.01\n",
        "#loss_weights = loss_weights.cuda()\n",
        "\n",
        "#distr weights\n",
        "data_distr = np.bincount(data.train_cut.reshape((-1)))\n",
        "ddd = torch.FloatTensor([1/(data_distr[i]*0.001) for i in range(135)]).cuda()\n",
        "criterion = nn.CrossEntropyLoss(ddd)\n",
        "#criterion = simple_criterion\n",
        "#criterion = simple_crit\n",
        "#criterion = nn.MSELoss()\n",
        "\n",
        "def accuracy(ys, ts):\n",
        "    ys = torch.argmax(ys, dim=1)\n",
        "    correct_prediction = torch.eq(ys, ts)\n",
        "    return torch.mean(correct_prediction.float())\n",
        "\n",
        "# Function to get label\n",
        "def get_labels(batch):\n",
        "    return get_variable(Variable(torch.from_numpy(batch['ts']).long()))\n",
        "\n",
        "# weight_decay is equal to L2 regularization\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)#, weight_decay=1e-5)\n",
        "\n",
        "# Initialize lists for training and validation\n",
        "train_iter = []\n",
        "train_loss, train_accs = [], []\n",
        "valid_iter = []\n",
        "valid_loss, valid_accs = [], []\n",
        "\n",
        "# Generate batches\n",
        "batch_gen = batch_generator(data,\n",
        "                                batch_size=batch_size,\n",
        "                                num_iterations=max_iter,\n",
        "                                seed=42,\n",
        "                                val_size=VALIDATION_SIZE)\n",
        "\n",
        "\n",
        "def get_input(batch):\n",
        "    return get_variable(Variable(torch.from_numpy(batch)))\n",
        "\n",
        "def get_variable(x):\n",
        "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
        "    if use_cuda:\n",
        "        return x.cuda()\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_numpy(x):\n",
        "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
        "    if use_cuda:\n",
        "        return x.cpu().data.numpy()\n",
        "    return x.data.numpy()\n",
        "\n",
        "\n",
        "# Train network\n",
        "net.train()\n",
        "train_idx = 0\n",
        "\n",
        "h = torch.randn(1, 500, lstm_hidden).cuda()\n",
        "c = torch.randn(1, 500, lstm_hidden).cuda()\n",
        "hidden = (h,c)\n",
        "\n",
        "for i, batch_train in enumerate(batch_gen.gen_train()):\n",
        "\n",
        "    if i % eval_every == 0:\n",
        "        # Do the validaiton\n",
        "        net.eval()\n",
        "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
        "\n",
        "        #batches = batch_gen.gen_valid()\n",
        "        for batch_valid, num in batch_gen.gen_valid():\n",
        "            output,_ = net(get_input(batch_valid['inputs']))\n",
        "            #print(\"hej\")\n",
        "            #print(output['out'])\n",
        "            #print(criterion(output['out'], get_input(batch_valid['targets'])))\n",
        "            val_losses += criterion(output['out'], get_input(np.squeeze(batch_valid['targets'])).long()) * num\n",
        "            val_accs += accuracy(output['out'], get_input(np.squeeze(batch_valid['targets']))) * num\n",
        "            val_lengths += num\n",
        "\n",
        "        # Divide by the total accumulated batch sizes\n",
        "        val_losses /= val_lengths\n",
        "        val_accs /= val_lengths\n",
        "        valid_loss.append(get_numpy(val_losses))\n",
        "        valid_accs.append(get_numpy(val_accs))\n",
        "        valid_iter.append(i)\n",
        "   \n",
        "        net.train()\n",
        "\n",
        "    # Train network\n",
        "    output,_ = net(get_input(batch_train['inputs']))\n",
        "\n",
        "    #labels_argmax = torch.max(get_labels(batch_train), 1)[1]\n",
        "    #validation_batch = get_variable(torch.transpose(torch.from_numpy(batch_train).long(), 0, 1))\n",
        "    #validation_batch = get_variable(torch.transpose(torch.from_numpy(batch_train).long(), 0, 1))\n",
        "    batch_loss = criterion(output['out'], get_input(np.squeeze(batch_train['targets'])).long())\n",
        "    train_iter.append(i)\n",
        "    train_loss.append(float(get_numpy(batch_loss)))\n",
        "    train_accs.append(float(get_numpy(accuracy(output['out'], get_input(batch_train['targets'])))))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    # Log i figure\n",
        "    if i % log_every == 0:\n",
        "        fig = plt.figure(figsize=(18, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_iter, train_loss, label='train_loss')\n",
        "        plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(train_iter, train_accs, label='train_accs')\n",
        "        plt.plot(valid_iter, valid_accs,\n",
        "                  label='valid_accs: ' + str(valid_accs[-1]))\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        clear_output(wait=True)\n",
        "   \n",
        "    if max_iter < i:\n",
        "        break\n",
        "\n",
        "\n",
        "#val = torch.max([])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpVNW1u_6leE"
      },
      "source": [
        "%debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtTJJ1r6V0cf"
      },
      "source": [
        "i = 2\n",
        "print(np.squeeze(batch_train[\"inputs\"][i]))\n",
        "np.squeeze(batch_train[\"targets\"][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsFqaTJHWgpE"
      },
      "source": [
        "\"\"\"\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "print(input)\n",
        "print(target)\n",
        "#output.backward()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "input = torch.from_numpy(np.array([[1.0,1.0,0.0]]))\n",
        "target = torch.from_numpy(np.array([1]))\n",
        "loss(input, target)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scr-RQeI_2mT"
      },
      "source": [
        "#notes_np = midi_conversion.convert_midi_to_numpy(\"maestro-v2.0.0/2008/MIDI-Unprocessed_04_R1_2008_01-04_ORIG_MID--AUDIO_04_R1_2008_wav--4.midi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrL4gncHCHxR"
      },
      "source": [
        "from midi_plotter import plot_midi, plot_num_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxcSontb6_bG"
      },
      "source": [
        "\"\"\"i = 4\n",
        "inp = batch_train['inputs'][i]\n",
        "tar = batch_train['targets'][i]\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZtxrSru8Kz2"
      },
      "source": [
        "#plot_midi(inp,show_plot=True, seq_length=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwH6C84o8IEV"
      },
      "source": [
        "#plot_midi(tar,show_plot=True, seq_length=TARGET_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFXKS_OCN2Vh"
      },
      "source": [
        "import midi_conversion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i75xpPW1N2Ex"
      },
      "source": [
        "r = random.randint(0,99)\n",
        "print(r)\n",
        "#inp_np = np.array(batch_train['inputs'])[r] # +np.full((128,1),5) #only one batch\n",
        "inp_np = batch_valid['inputs'][r] # +np.full((128,1),5) #only one batch\n",
        "print(np.squeeze(inp_np))\n",
        "net.eval()\n",
        "all_removed = []\n",
        "for i in range(1000):\n",
        "    #inp = torch.from_numpy(np.array([inp_np])).float().cuda()\n",
        "    inp = get_input(np.array([inp_np]))\n",
        "    outp = net(inp)['out']\n",
        "    outp = outp.cpu().detach()\n",
        "    pred = torch.argmax(outp).numpy()\n",
        "    #print(\"pred\",pred)\n",
        "    all_removed.append(inp_np[0])\n",
        "    inp_np = np.roll(inp_np,-1,0)\n",
        "    inp_np[-1] = [pred]\n",
        "\n",
        "np.squeeze(inp_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbOruclwo4RZ"
      },
      "source": [
        "num_seq = [int(num) for num in np.squeeze(inp_np)]\n",
        "all_removed = [int(num) for num in all_removed]\n",
        "num_seq = all_removed+num_seq\n",
        "midi_conversion.convert_num_seq_to_midi(num_seq,output_name=\"GENERATED\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX3ohxHzmsA9"
      },
      "source": [
        "%debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOB_mCaYCoEv"
      },
      "source": [
        "###### OUTPUT NETWORK TEST ###########\n",
        "#get an input and convert\n",
        "i = 4\n",
        "inp = batch_train['inputs']\n",
        "tar = batch_train['targets']\n",
        "inp = torch.from_numpy(np.array(inp)).float().cuda()\n",
        "#inp = torch.from_numpy(np.array(inp)).float()\n",
        "inp.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj21BfmbJLCg"
      },
      "source": [
        "#get networks output\n",
        "net.eval()\n",
        "outp = net(inp)['out']\n",
        "outp = outp.cpu().detach()\n",
        "#outp = torch.argmax(outp,dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl6MUE8ouxBX"
      },
      "source": [
        "tar = torch.from_numpy(tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdJwaf3UvF1K"
      },
      "source": [
        "torch.mean(torch.eq(outp,tar).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAkgfziSvcHM"
      },
      "source": [
        "torch.argmax(outp,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMGGn95IV-1"
      },
      "source": [
        "#analyse\n",
        "print(\"Shape of outp:\",outp.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWKwaIc0txvZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4T8CQg1by_Z"
      },
      "source": [
        "tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuvvki8BV1vE"
      },
      "source": [
        "outp[259]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uvrxqcvXUbF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "x = list(range(458))\n",
        "y = outp\n",
        "ax.bar(x,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt8hovlwbkSB"
      },
      "source": [
        "outp = np.where(outp > 0.9, 1, 0)\n",
        "plot_num_seq(outp,show_plot=True, seq_length=TARGET_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARLxP516JnrX"
      },
      "source": [
        "#plotting target again\n",
        "plot_midi(tar,show_plot=True, seq_length=TARGET_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYmbVC0jgp6b"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvzy-R7bJyQy"
      },
      "source": [
        "plt.hist(de, bins = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALDQfytPfpvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}